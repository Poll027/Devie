# -*- coding: utf-8 -*-
"""14-06

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ztdo8a2HSGTXkoGTMnaUYOko5T48bmiV
"""

!pip install -q transformers accelerate bitsandbytes gradio

from huggingface_hub import notebook_login
notebook_login()


import json
import re
import gc
import os
import torch
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig
from peft import PeftModel
from accelerate import init_empty_weights, load_checkpoint_and_dispatch

@dataclass
class CareerRecommendation:
    name: str
    score: float
    description: str
    priority: int

class CareerAssessmentChatbot:
    def __init__(self):
        """Initialize the chatbot"""
        self.conversation_history = []
        self.trait_scores = {}
        self.assessment_complete = False
        self.model = None
        self.tokenizer = None
        self.model_loaded = False
        self.awaiting_user_response = True
        self.question_count = 0
        self.max_questions = 25
        self.context = ""
        self.prev_questions = []
        self.career_weights = {}
        self.traits = [
            'database_fundamentals', 'computer_architecture', 'distributed_computing_systems',
            'cyber_security', 'networking', 'software_development', 'programming_skills',
            'project_management', 'computer_forensics_fundamentals', 'technical_communication',
            'ai_ml', 'software_engineering', 'business_analysis', 'communication_skills',
            'data_science', 'troubleshooting_skills', 'graphics_designing', 'openness',
            'conscientiousness', 'extraversion', 'agreeableness', 'emotional_range',
            'conversation', 'openness_to_change', 'hedonism', 'self_enhancement',
            'self_transcendence', 'creativity', 'analytical_thinking', 'problem_solving',
            'attention_span', 'curiosity', 'iq_estimate', 'adaptability', 'team_collaboration',
            'tool_adaptability', 'pattern_recognition', 'learning_style', 'innovation_drive',
            'work_life_balance', 'people_facing'
        ]
        self.tech_careers = [
            "Software Development", "Data Science", "Cybersecurity", "UX/UI Design", "IT Support",
            "Cloud Engineering", "DevOps Engineering", "AI/ML Engineering", "Mobile Development",
            "Full Stack Development", "Frontend Development", "Backend Development", "Game Development",
            "Blockchain Development", "Product Management", "Data Engineering", "Network Engineering",
            "Database Administration", "Quality Assurance", "Technical Writing", "Digital Marketing",
            "Business Analysis", "Site Reliability Engineering", "Robotics Engineering"
        ]
        self.workflow_instructions = """
        You are Devy, Devspaceâ€™s AI-powered career advisor chatbot. You guide users step-by-step through a multi-phase tech career exploration to recommend ideal paths.

        ---
        1. USER ONBOARDING â€“ Profiling Introduction
        â€¢ Greet warmly and ask for basic info (name, age, education, tech experience).
        â€¢ Discover top three academic subjects and what aspects excite the user.

        2. USER REVIEW â€“ Web-Based Similarity & Preference Check
        â€¢ Suggest five common tech paths for users like them.
        â€¢ Ask casual, scenario-based questions (e.g. â€œIf you could redesign your favorite websiteâ€™s homepage, what would you change first?â€).
        â€¢ Internally track sentiment (positive/neutral/negative) for each pathâ€”do not display.

        3. PROGRESSIVE INSIGHT MAPPING â€“ Live Understanding of Tendencies
        â€¢ After each response, tag inferred traits (e.g. pattern_recognition, team_collaboration).
        â€¢ Continue to refine which career paths are growing stronger.

        4. ADAPTIVE QUESTION GENERATION â€“ 15â€“25 Scenario & Behavior Questions
        â€¢ Based on data quality and emerging patterns, ask 15â€“25 total.
        â€¢ Make questions open-ended, relatable to everyday life or school.
        â€¢ Include lightly technical scenarios only if the user shows tech fluency.

        5. FINAL SCORING & RECOMMENDATION OUTPUT
        â€¢ The five target domains (internally scored) are:
            - Software Development
            - Data Science
            - Cybersecurity
            - UX/UI Design
            - IT Support
        â€¢ Each domain is scored on these five core criteria (subâ€‘scores must sum to 100% for that domain):
            1. Creativity & Innovation
            2. Problem-Solving & Logical Thinking
            3. Collaboration & Communication
            4. Learning Style & Adaptability
            5. Alignment with Career Values & Interests
        â€¢ Plus extended trait dimensions to enrich scoring: IQ Estimate, Openness, Pattern Recognition, Team Collaboration, Workâ€‘Life Balance, Tool Adaptability, Curiosity
        """

    def get_custom_body(self):
        """Returns user-facing introduction"""
        return "Hi! I'm Devy, your career advisor from Devspace. I'm here to help you find the best tech career paths by understanding your personality and preferences."

    def build_career_question_prompt(self, context, prev_questions, last_response, history):
        """Constructs prompt to generate only a single question"""
        history_tail = "\n".join(history[-3:])
        prompt = f"""
        You are Devy, Devspaceâ€™s AI-powered career advisor chatbot. Your task is to ask the user one open-ended, scenario-based question to assess their traits for tech career recommendations. Generate only the question, ending with a question mark (?), and do not simulate the user's response or include additional text.

        Conversation context:
        {context}

        Previous questions:
        {prev_questions}

        Last user response:
        {last_response}

        Conversation history (last 3 messages):
        {history_tail}

        Ask your next question:
        """
        return prompt

    def load_model(self):
        """Load the fine-tuned model with optimized configuration using accelerate"""
        try:
            print("Starting model loading process...")
            if hasattr(self, 'model') and self.model is not None:
                del self.model
            if hasattr(self, 'tokenizer') and self.tokenizer is not None:
                del self.tokenizer
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                print(f"CUDA available. GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

            base_model_id = "unsloth/Meta-Llama-3.1-8B"
            lora_adapter_id = "Poll027/my-lora-llama"

            print("Loading tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(base_model_id)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            print("Loading base model with accelerate...")
            with init_empty_weights():
                config = AutoConfig.from_pretrained(base_model_id, trust_remote_code=True)
                base_model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)

            base_model = AutoModelForCausalLM.from_pretrained(
                base_model_id,
                torch_dtype=torch.float16,
                trust_remote_code=True,
                device_map="auto"
            )

            print("Loading LoRA adapter...")
            self.model = PeftModel.from_pretrained(
                base_model,
                lora_adapter_id,
                torch_dtype=torch.float16
            )
            print("Warming up model...")
            dummy_input = self.tokenizer("Warmup", return_tensors="pt").to("cuda")
            with torch.no_grad():
                _ = self.model.generate(**dummy_input, max_new_tokens=1)

            self.model.eval()
            self.model_loaded = True
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            print("Model loaded successfully!")
            return "Model loaded successfully!"
        except Exception as e:
            print(f"Error loading model: {str(e)}")
            self.model_loaded = False
            if hasattr(self, 'model') and self.model is not None:
                del self.model
                self.model = None
            if hasattr(self, 'tokenizer') and self.tokenizer is not None:
                del self.tokenizer
                self.tokenizer = None
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            raise e

    def model_inference(self, prompt: str, max_length: int = 180) -> str:
        """Generate response using the loaded model with optimized settings"""
        if not self.model_loaded:
            return "Model not loaded. Please load the model first."

        try:
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=1024,
                padding=True
            )
            device = next(self.model.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}

            with torch.no_grad():
                outputs = self.model.generate(
                    inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    max_new_tokens=max_length,
                    temperature=0.6,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    use_cache=True,
                )

            response = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            response = response.strip().split('\n')[0]
            if not response.endswith('?') or any(keyword in response for keyword in ["Use the", "- ", "Insight"]):
                return ""
            return response
        except Exception as e:
            print(f"Error during inference: {str(e)}")
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            return ""

    def extract_traits_from_response(self, user_input: str) -> Dict[str, float]:
        """Extract traits from user response (placeholder)"""
        return {'team_collaboration': 7.0, 'problem_solving': 6.5}

    def start_conversation(self):
        """Start the conversation with an intro and first question"""
        intro = self.get_custom_body()
        self.conversation_history.append(f"Assistant: {intro}")
        prompt = self.build_career_question_prompt(
            context=self.context,
            prev_questions="",
            last_response="",
            history=[intro]
        )
        try:
            first_question = self.model_inference(prompt)
            if not first_question:
                first_question = "What do you enjoy most about solving problems?"
            self.conversation_history.append(f"Assistant: {first_question}")
            self.prev_questions.append(first_question)
            self.question_count = 1
            return f"{intro}\n\n{first_question}"
        except Exception as e:
            fallback = "What do you enjoy most about solving problems?"
            self.conversation_history.append(f"Assistant: {fallback}")
            self.prev_questions.append(fallback)
            self.question_count = 1
            return f"{intro}\n\n{fallback}"

    def process_user_input(self, user_input: str) -> str:
        """Process user input and generate next question"""
        extracted_traits = self.extract_traits_from_response(user_input)
        for trait, score in extracted_traits.items():
            if trait in self.trait_scores:
                self.trait_scores[trait] = (self.trait_scores[trait] + score) / 2
            else:
                self.trait_scores[trait] = score
        if self.question_count >= self.max_questions:
            return self.generate_final_recommendations()
        prompt = self.build_career_question_prompt(
            context=self.context,
            prev_questions="\n".join(self.prev_questions),
            last_response=user_input,
            history=self.conversation_history
        )
        try:
            next_question = self.model_inference(prompt)
            if not next_question:
                fallback_questions = [
                    "How do you prefer to learn new skills?",
                    "What motivates you most in your work?",
                    "How do you handle challenging problems?"
                ]
                next_question = fallback_questions[self.question_count % len(fallback_questions)]
            self.conversation_history.append(f"Assistant: {next_question}")
            self.prev_questions.append(next_question)
            self.question_count += 1
            return next_question
        except Exception as e:
            fallback_questions = [
                "How do you prefer to learn new skills?",
                "What motivates you most in your work?",
                "How do you handle challenging problems?"
            ]
            response = fallback_questions[self.question_count % len(fallback_questions)]
            self.conversation_history.append(f"Assistant: {response}")
            self.prev_questions.append(response)
            self.question_count += 1
            return response

    def generate_final_recommendations(self) -> str:
        """Generate final career recommendations"""
        if not self.model_loaded:
            return "Model not loaded for final analysis."
        total_score = sum(self.trait_scores.values())
        recommendation_prompt = f"""
        Based on these trait scores, recommend the top 3 tech careers with scores (0-10) and explanations:

        Trait Scores: {json.dumps(self.trait_scores, indent=2)}
        Total Score: {total_score}
        Available Careers: {', '.join(self.tech_careers)}

        Provide recommendations in JSON format:
        {{
            "recommendations": [
                {{"career": "Career Name", "score": 8.5, "explanation": "Why this career fits"}},
                {{"career": "Career Name", "score": 8.0, "explanation": "Why this career fits"}},
                {{"career": "Career Name", "score": 7.5, "explanation": "Why this career fits"}}
            ]
        }}
        """
        try:
            model_response = self.model_inference(recommendation_prompt, max_length=400)
            json_match = re.search(r'\{.*\}', model_response, re.DOTALL)
            if json_match:
                recommendations_data = json.loads(json_match.group())
                result = "ðŸŽ‰ **CAREER ASSESSMENT COMPLETE** ðŸŽ‰\n\n"
                result += "Here are your top 3 tech career recommendations:\n\n"
                for i, rec in enumerate(recommendations_data.get("recommendations", []), 1):
                    result += f"**#{i}. {rec['career']}** (Score: {rec['score']}/10)\n"
                    result += f"  {rec['explanation']}\n\n"
                self.assessment_complete = True
                return result
            else:
                return self.fallback_recommendations()
        except Exception as e:
            print(f"Error generating recommendations: {str(e)}")
            return self.fallback_recommendations()

    def fallback_recommendations(self) -> str:
        """Fallback recommendation method"""
        career_scores = {}
        for career in self.tech_careers[:10]:
            score = sum(self.trait_scores.get(trait, 5.0) for trait in ['programming_skills', 'problem_solving', 'creativity']) / 3
            career_scores[career] = round(score, 1)
        top_3 = sorted(career_scores.items(), key=lambda x: x[1], reverse=True)[:3]
        result = "ðŸŽ‰ **CAREER ASSESSMENT COMPLETE** ðŸŽ‰\n\n"
        result += "Here are your top 3 tech career recommendations:\n\n"
        for i, (career, score) in enumerate(top_3, 1):
            result += f"**#{i}. {career}** (Score: {score}/10)\n"
            result += f"  This career aligns well with your skills and interests.\n\n"
        self.assessment_complete = True
        return result

    def reset_assessment(self):
        """Reset the assessment"""
        self.conversation_history = []
        self.trait_scores = {}
        self.assessment_complete = False
        self.question_count = 0
        self.context = ""
        self.prev_questions = []

def create_gradio_interface():
    """Create Gradio interface"""
    chatbot = CareerAssessmentChatbot()

    def load_model_interface():
        try:
            message = chatbot.load_model()
            return message, gr.update(interactive=True), gr.update(interactive=True)
        except Exception as e:
            error_msg = f"Error loading model: {str(e)}"
            print(error_msg)
            return error_msg, gr.update(interactive=False), gr.update(interactive=False)

    def start_new_chat():
        if not chatbot.model_loaded:
            return [[None, "Please load the model first."]], ""
        try:
            chatbot.reset_assessment()
            initial_response = chatbot.start_conversation()
            return [[None, initial_response]], ""
        except Exception as e:
            return [[None, f"Error starting chat: {str(e)}"]], ""

    def respond(user_message, chat_history):
        if not chatbot.model_loaded:
            chat_history.append([user_message, "Please load the model first."])
            return chat_history, ""
        if not user_message.strip():
            return chat_history, ""
        try:
            bot_response = chatbot.process_user_input(user_message)
            chat_history.append([user_message, bot_response])
            return chat_history, ""
        except Exception as e:
            chat_history.append([user_message, f"Error: {str(e)}"])
            return chat_history, ""

    with gr.Blocks(title="Tech Career Assessment", theme=gr.themes.Soft()) as interface:
        gr.Markdown("# Tech Career Assessment Chatbot")
        gr.Markdown("**Discover your ideal tech career through AI-powered conversation**")
        with gr.Row():
            load_button = gr.Button("Load Model")
            model_status = gr.Textbox(label="Model Status", interactive=False)
        with gr.Row():
            chatbot_interface = gr.Chatbot(label="Career Assessment Chat", height=400, show_label=True)
            user_input = gr.Textbox(placeholder="Type your response here...", label="Your Answer", lines=2, interactive=False)
        with gr.Row():
            send_button = gr.Button("Send")
        load_button.click(fn=load_model_interface, outputs=[model_status, user_input, send_button])
        send_button.click(fn=respond, inputs=[user_input, chatbot_interface], outputs=[chatbot_interface, user_input])
        interface.load(fn=start_new_chat, outputs=[chatbot_interface, user_input])
    return interface

if __name__ == "__main__":
    print("Starting Tech Career Assessment Chatbot...")
    interface = create_gradio_interface()
    interface.launch(debug=True)